\documentclass[11pt]{article}
\usepackage{amsfonts,amssymb,amsthm,eucal,amsmath}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{latexsym,url}
\usepackage{array}
\usepackage{subfig}
\usepackage{comment}
\usepackage{color}
\usepackage{hyperref}

\newcommand{\myspace}{\vspace{.1in}\noindent}
\newcommand{\mymyspace}{\vspace{.1in}}
\usepackage[inner=30mm, outer=30mm, textheight=225mm]{geometry}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{defn}[theorem]{Definition}
\newtheorem{notn}[theorem]{Notation}
\newtheorem{cond}[theorem]{Condition}
\newtheorem{ex}[theorem]{Example}
\newtheorem{rmk}[theorem]{Remark}
\newcommand{\co}{\negthinspace :}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\CP}{\mathbb{CP}}
\newcommand{\PSL}{\mathrm{PSL}_2(\mathbb{C})}
\newcommand{\area}{\operatorname{area}}
\newcommand{\rand}{\operatorname{rand}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\nt}{\negthinspace}
\newcommand{\TODO}{{\color{red} TODO}}

\title{A deterministic pseudorandom perturbation scheme for arbitrary polynomial predicates}
\author{Geoffrey Irving\thanks{Email: irving@naml.us, Otherlab, San Francisco, CA, United States}
\and Forrest Green\thanks{\TODO}}
\date{Version 1, \today}

\begin{document}
\maketitle

\begin{abstract}
We present a symbolic perturbation scheme for arbitrary polynomial geometric predicates, which combines the benefits of
Emiris and Canny's simple randomized linear perturbation scheme with Yap's multiple infinitesimal scheme for general predicates.
Like the randomized scheme, our method accepts black box polynomial functions as input.  For nonmaliciously chosen predicates,
our method is as fast as the linear scheme, and in particular scales reasonably with the degree of the polynomial even for fully
degenerate input.  Like Yap's scheme, the computed sign is deterministic, never requiring an algorithmic restart (assuming a
high quality pseudorandom generator), and works for arbitrary predicates with no knowledge of their structure.  We also apply
our technique to exactly or nearly exactly rounded constructions that work correctly for degenerate input, using l'H\^opital's
rule to compute the necessary singular limits.  Our prototype implementation is available as open source, together with example
algorithms for Delaunay triangulation and Boolean operations on polygons and circular arc polygons in the plane.
\end{abstract}

\section{Introduction}

Symbolic perturbation is a standard technique in computational geometry for avoiding degeneracies by
adding an infinitesimally small perturbation to the inputs of a geometric algorithm.  The technique was introduced by
\cite{edelsbrunner1990simulation}, with refinements in \cite{yap1990symbolic}, \cite{emiris1992efficient}, \cite{emiris1995general},
and \cite{seidel1998nature}.  Consider a geometric function $G : \R^N \to S$ mapping input coordinates $x \in \R^N$ into
some finite set $S$.  Examples of $G(x)$ include Delaunay triangulation, arrangements of lines or circles, or Boolean operations
on shapes.  We will assume $G(x)$ can be computed using an algorithm that makes queries its input $x$ only through the
signs of various polynomials $f(x)$ with integer coefficients, each representing a geometric predicate such as
``is this triangle counterclockwise?'' or ``do two circles intersect inside a third circle?''.  If $f(x) = 0$, the algorithm
either fails due to ambiguity or requires special logic to handle the degeneracy.

We describe symbolic perturbation in the framework of nonstandard analysis; see \cite{yap1990symbolic}, \cite{emiris1995general},
and \cite{seidel1998nature} for the geometric meaning of this approach.  To extend $G(x)$ to degenerate inputs, we introduce one
or more positive infinitesimal quantities $\epsilon_1, \epsilon_2, \ldots$, with $0 < \epsilon_i < 1/n$ for all $i,n > 0$.  If
we introduce more than one infinitesimal, we define a relative ordering of the different monomials $\epsilon_1^{d_1} \epsilon_2^{d_2} \cdots$;
the simplest is lexicographic ordering where $\epsilon_i^n > \epsilon_{i+1}$ for all $i,n > 0$.
We then form an infinitesimal perturbation $\delta \in \R[\epsilon_1,\epsilon_2, \ldots]^N$ from
linear combinations of the infinitesimals, and evaluate
\begin{align*}
G'(x) = G(x+\delta).
\end{align*}
In detail, whenever the algorithm asks for the sign of $f(x)$ for some integer coefficient polynomial $f$, we instead compute
$f(x+\delta)$, which is a multivariate polynomial in the infinitesimals.  The sign of $f(x+\delta)$ is the sign of the ``least infinitesimal''
nonzero monomial coefficient of this polynomial.

We now distinguish between three existing symbolic perturbation schemes that can be expressed in this framework, and discuss their advantages
and disadvantages:

\begin{enumerate}
\item Yap's deterministic scheme \cite{yap1990symbolic}: Introduce one infinitesimal $\epsilon_i$ per input coordinate $x_i$, and let $\delta_i = \epsilon_i$.  This
corresponds to evaluating $f(x_1+\epsilon_1, x_2+\epsilon_2, \ldots)$.  Since each coordinate has its own infinitesimal, $f(x+\delta)$ has at least
one nonzero monomial unless $f$ is identically zero, so the scheme produces a nonzero sign for all polynomials.  Unfortunately, a degree $d$ polynomial
$f$ results in an $f(x+\delta) \in \R[\epsilon_1,\epsilon_2, \ldots]$ with up to $\binom{n + d}{n}$ monomial terms where $n$ is the number of input
coordinates used by $f$, which is worst case exponential in the degree in the predicate.  For extremely degenerate input, we may have to evaluate a large
number of coefficents before finding a nonzero.

\item Emiris and Canny's deterministic linear scheme \cite{emiris1992efficient}: Group the input coordinates into $n$ $k$-vectors based on the dimension
$k$ of the geometric space as $x_{i,j}$, $1 \le i \le n$, $1 \le j \le k$, introduce a single infinitesimal $\epsilon$, and write
$$\delta_{i,j} = \epsilon (i^j \operatorname{mod} p)$$
where $p > n$ is a prime.  They show that this scheme produces a nonzero sign for simplex orientation tests up to dimension $k$ and for
the incircle tests used in Delaunay triangulation.  However, as discussed in \cite{seidel1998nature}, extending this technique to more complicated predicates
is difficult.

\item Emiris and Canny's randomized linear scheme \cite{emiris1995general}: Again introduce a single infinitesimal $\epsilon$, but now set
$\delta_i = \epsilon y_i$ using random coefficients $y_i$ chosen from some space $Y$.  By the Schwartz-Zippel lemma \cite{schwartz1980fast}, $f(x+\delta)$
will be nonvanishing as a polynomial in $\epsilon$ with probability at least $1 - d/|Y|$, where $d$ is the degree of the polynomial.  Unfortunately, what we
actually need is for \emph{all} polynomials evaluated during the algorithm to not vanish, which reduces the probability of success to
$(1 - d/|Y|)^T$ where $T$ is the number of branches required.  \cite{emiris1995general} thus show that their randomized scheme is very efficient in the
algebraic computation model, but suffers from a worst case cubic slowdown in the bit computation model due to the large $|Y|$ required.  For some algorithms
it is possible to reduce this slowdown by restarting only part of the algorithm, but this adds significant complexity (in the authors' experience!).
\end{enumerate}

In addition, \cite{burnikel1994degeneracy} note that a fixed deterministic perturbation may turn high degenerate input into worst case behavior for algorithms
like convex hull: ignoring the $\operatorname{mod} p$, the deterministic linear scheme produces a convex hull of size $n^{\lceil d/2 \rceil}$ when all input
points are at the origin.  We believe this also applies to Yap's scheme, and may also arise with the modular deterministic linear scheme.

To summarize: Yap's deterministic scheme and the randomized linear scheme work for arbitrary polynomial predicates, but suffer from unfortunate performance penalties.
The randomized linear scheme occasionally requires a restart of all or part of the computation, adding extra complexity to the surrounding algorithm
especially if multiple computations are chained together (possibly with user interaction in between!).  The deterministic linear scheme is ideal when it works,
but requires special analysis to verify correctness for each predicate.

Our contribution is to combine the advantages of each of the above methods.

\section{A deterministic pseudorandom perturbation}

Our approach is to introduce an infinite sequence of infinitesimals $\epsilon_1, \epsilon_2, \ldots$, choose deterministic pseudorandom coefficients $y_1, y_2, \ldots$
with $y_{k,i} = \rand(k,i)$, and set
$$\delta = \epsilon_1 y_1 + \epsilon_2 y_2 + \ldots.$$
Here $\rand$ is a deterministic pseudorandom generator with random access capability.  Our implementation uses the Threefry generator of
\cite{salmon2011random}, with
$$\rand : [0,2^{128}) \times [0,2^{128}) \to [0,2^{32}).$$
We order the infinitesimals largest first, so that $\epsilon_i^d > \epsilon_{i+1}$ for all $d > 0$.  As in Yap's scheme, this ordering lets us add one term of the
perturbation series at a time, evaluating
\begin{align*}
f_0 &= f(x) \\
f_1 &= f(x + \epsilon_1 y_1) \\
f_2 &= f(x + \epsilon_1 y_1 + \epsilon_2 y_2) \\
f_3 &= f(x + \epsilon_1 y_1 + \epsilon_2 y_2 + \epsilon_3 y_3) \\
\vdots
\end{align*}
and stopping as soon as we arrive at a nonzero polynomial $f_k(\epsilon_1. \ldots, \epsilon_k)$.
To compute each $f_k$, we use a black box function for $f(x)$ to evaluate $f_k(\epsilon_1, \ldots, \epsilon_k)$ for the $\binom{k+d}{k}$ nonnegative tuples $(\epsilon_1, \ldots, \epsilon_k)$
s.t.\ $\epsilon_1 + \cdots + \epsilon_k \le d$, as discussed in \cite{neidinger2009multivariable} (see also \autoref{polynomial}).  If any values are nonzero, we use multivariate
polynomial interpolation to recover the $\binom{k+d}{k}$ coefficients of $f_k$, and return the sign of the least infinitesimal nonzero term.

We show that the computational cost is dominated by the first perturbation term even for arbitrarily degenerate input, as long as the range $Y$ of the random generator
satisfies $d^3 \ll |Y|$.  In other words, our scheme has the same cost as the simple linear scheme.  To see this, note that $f_k$ can be zero only if $f(x + y_1), \ldots, f(x + y_k)$ are zero,
by setting one $\epsilon_i$ to one and the others to zero.  Thus, if the polynomial predicate $f(x)$ is not identically zero, the Schwartz-Zippel lemma gives
$$\Pr(f_k = 0) \le \frac{d^k}{|Y|^k}.$$
The sizes of the lattice points on which we evaluate $f$ grow slowly with $k$, so the cost of a single polynomial evaluation is effectively $O(1)$ where the constant depends on the
polynomial, even in the bit complexity model.  Similarly, the sizes of the numbers used for multivariate interpolation also grow slowly with $k$, so the cost of multivariate
interpolation at level $k$ is $O\left(\binom{d + k}{k}^2\right)$ (see Appendix \TODO).  Thus, the expected cost of the perturbation scheme is
\begin{align} \label{cost}
\sum_{k = 0}^\infty \Pr(f_k = 0) O\left(\binom{d + k + 1}{k + 1}^2\right) \le \sum_{k = 0}^\infty \frac{d^k}{|Y|^k} O(d^{2k+2}) = O\left(d^2 \sum_{k = 0}^\infty \frac{d^{3k}}{|Y|^k} \right)
\end{align}
which is a convergent geometric series whenever $d^3 < |Y|$.  In practice, $d^3 \ll |Y|$; for $|Y| = 2^{32}$ terms with $k \ge 2$ contribute less than $1/4000$th of the expected cost
for polynomials up to degree $100$.  We emphasize that this bound is independent of the input $x$, and therefore holds even for maliciously chosen input data.  However, we do
assume that $\rand$ behaves as a strong random source, and in particular that the polynomials $f(x)$ are not chosen with knowledge of $\rand$.\footnote{Though maliciously
choosing $f(x)$ so that $f_1 = f_2 = 0$ is quite useful for unit testing purposes.}

Thus, our method has the same complexity as the deterministic linear scheme, but like Yap's scheme and the randomized linear scheme it works on arbitrary polynomials.  As in the
randomized scheme, the perturbation does not create any ``worst case'' behavior not already present in the input data.  Since the occasional random fallbacks occur one
polynomial at a time, the outer structure of a geometric algorithm is blissfully unaware that randomness is used internally, and in particular we avoid poor bit complexity
scaling when evaluating many predicates over the course of an algorithm.

\section{Other approaches}

Since the original introduction of the symbolic perturbation method several alternative schemes have been introduced for treating degeneracies in numerical algorithms.
All of these approaches seem to require some algorithm or predicate specific treatment, which complicate the process of developing and especially testing new algorithms.
However, the algorithm specific approaches may be superior to a general approach such as ours when they apply, either by avoiding the slowdown of occasional exact arithmetic
entirely, by treating degenerate cases faster (our approach introduces a slowdown of $O(d^2)$ for the first perturbation level), or by computing the true exact answer rather
than a perturbed answer.

Perhaps the most natural approach to treating to degeneracies is to manually extend the definition of $G(x)$ to degenerate cases, and write algorithms which treat these cases
directly.  For example, in an arrangement of lines, intersections of three or more lines can be detected and represented as higher degree vertices in the arrangement graph.
\cite{burnikel1994degeneracy} argues that perturbation is slower and more complicated to implement than simply handling degeneracies directly, and presents two degeneracy-aware
algorithms as evidence.  We believe our method reduces the implementation complexity of symbolic perturbation, but agree that a tailored algorithm is likely faster on highly
degenerate input.  Unlike the deterministic symbolic perturbation schemes, an algorithm built on our method will treat fully degenerate data as purely random data, in particular
avoiding the worst case behavior of convex hull discussed in \cite{burnikel1994degeneracy}.

The \emph{controlled perturbation} approach of \cite{halperin1998perturbation} applies a small finite perturbation to the input points to avoid degeneracies, allowing the rest
of the algorithm to run with inexact floating point arithmetic.  Input points (spheres in their case) are processed one at a time, perturbing each new input to avoid degeneracies
against all previous inputs.  Controlled perturbation requires a careful enumeration of the possible degeneracies that may arise, and a careful choice of the finite tolerance
required for the algorithm to run safely.  A good tolerance bound may be computed with numerical analysis techniques as in \cite{halperin2004controlled}, at the cost of significant
algorithmic-specific analysis.  The main advantage of their approach over ours is speed: the majority of their algorithms avoid all exact arithmetic and even all interval arithmetic
or other filters.  We note that if degeneracies are pervasive and the cost of exact arithmetic times $O(d^2)$ is too large, an input to a symbolically perturbed algorithm can be randomly
jittered by a small amount, reducing the practical overhead to the cost of interval analysis filtering without affecting correctness.  Unlike controlled perturbation, this requires
no algorithm specific analysis.

\cite{devillers2012qualitative} present \emph{qualitative symbolic perturbation}, which replaces the algebraic perturbations used in previous perturbation schemes (and ours)
by a sequence of carefully chosen, geometrically meaningful perturbations.  Their approach avoids the $O(d^2)$ slowdown of our method caused by (usually univariate) polynomial interpolation,
and is likely significantly faster when it applies.  However, the geometric perturbations and the analysis of their effect on the predicates must be performed separately for each
predicate, which complicates the design of algorithms and is a likely source of complexity during implementation and debuggging.  Moreover, since the perturbations depend on the
algorithm, chaining two algorithms together requires adjusting the perturbations to be compatible.  Interestingly, their approach shares with ours (and indeed
with Yap's) the idea of a sequence of ``increasingly small'' perturbations, applied one at a time until a nonsingular result is obtained.

Finally, we address a common complaint against symbolic perturbation, namely that a complicated postprocessing step is required to obtain the ``exact'' answer from the
perturbed result.  We believe that in the vast majority of applications which might benefit from a more exact answer, what it is actually desired is something much
stronger: a finitely nondegenerate geometric structure with no small triangles, sharp angles, etc.\  Thus, in practice, an exactly computed geometric result may need to
be followed by mesh simplification, smoothing, etc.\ (possibly followed by further exact algorithms), and that this extra work is the same whether or not exact degeneracies are
treated specially.

\section{Implementation}

A C++ implementation of our symbolic perturbation technique is available under a BSD license at \url{https://github.com/otherlab/core/tree/exact} (see commit 7baac83f65105b$\ldots$
for the version at the time of this writing (\TODO: update)).  The code includes three algorithms built on top of the perturbation core: Delauany triangulation,
Boolean operations on polygons, and Boolean operations on polygons built from circular arcs.  We plan to expand the set of implemented algorithms and use them for various tasks
in CAD/CAM such as shape decomposition for manufacturing and motion planning (\TODO: Add citations).

For simplicity and speed, our implementation quantizes all input coordinates to the integer range $[-2^{53},2^{53}]$, the largest range of integers exactly representable in
double precision.  This allows us to use fast interval arithmetic filters (\TODO: cite), falling back to exact evaluation using GMP if the filter fails (\TODO: cite), falling back
to symbolic perturbation if the exact answer is zero.  The polynomial is provided as a black box evaluation routine (see \verb+exact/perturb.h+ in the code).  For multivariate
interpolation we evaluate $f_k(\epsilon_1, \ldots, \epsilon_k)$ on our fixed set of $(\epsilon_1, \ldots, \epsilon_k)$ tuples, use the algorithm of \cite{neidinger2009multivariable}
to map into the Newton basis, then expand into the monomial basis.  It is possible to perform all computations required for polynomial interpolation using integers only; see \autoref{polynomial}.

In addition to computing the perturbed signs of polynomial predicates, we use our scheme to compute exactly rounded perturbed constructions.  Given a rational function $f(x)/g(x)$
with $g(x) = 0$, we compute the perturbation series $g_1, g_2, \ldots$ until we find a nonzero $g_k$, compute the perturbed numerator $f_k$, then evaluate the perturbed result as
the ratio of the matching least infinitesimal nonzero term in $f_k$ and $g_k$.  In a correct algorithm this ratio will always be finite, in that $f_k$ will never contain a nonzero term
larger than $g_k$, but it is easy to detect this case and throw an exception as an aid to debugging.  Note that the ratio of matching least infinitesimal terms is exactly l'H\^opital's
rule for computing limits.  Finally, the ratio is rounded to the nearest integer.  We can similarly compute $\sqrt{f(x)/g(x)}$ by evaluating the limit of the ratio as a rational and
taking an exactly rounded square root.

We emphasize that these perturbed constructions are (except for the final rounding) geometrically consistent with the rest of the algorithm, and in particular obey any geometric
invariants that apply in the exact case (again, except for the rounding).  For example, the union of a convex polygon with itself will remain nearly convex, and in particular will
avoid (all but extremely tiny) foldovers that might result from performing constructions with floating point arithmetic when an algorithm completes.  Moreover, since they
are guaranteed to be within distance $1/2$ of the true answer, they can be fed back into the same algorithm as tight interval bounds without fear of introducing inconsistencies.  Our
circular arc Boolean code makes use of this to perform more accurate interval-based filtering.  For example, when comparing the $y$ coordinates of different intersections of circles,
we precompute the rounded intersections and avoid costly polynomial evaluation if the rounded coordinates differ.

Debugging and testing the symbolically perturbed algorithms we have implemented so far has been quite a pleasant experience.  Once the perturbation core itself is trusted,
bugs in the surrounding algorithm necessarily manifest on a set of positive measure, and therefore are likely to be found by running the algorithm on random input.  In contrast, an
algorithm which handles degeneracies specially or tailors the perturbation to the predicates involved must actually test each kind of degeneracy when debugging the algorithm.
Any speedup logic such as interval filtering can be easily checked by including a compile time flag to unconditionally evaluate both fast and slow paths.  This tests both the
correctness of the filter and the correctness of the predicate, which is important for complicated polynomial predicates.

In a correct geometric algorithm, no polynomial passed to symbolic perturbation will be identical zero; this would correspond to a fundamentally degenerate question such as
``Is the triangle $(x_7,x_7,x_7)$ counterclockwise?''.  However, it is extremely convenient for debugging to detect these cases and produce useful output.  Therefore,
if both $f_1$ and $f_2$ are identically zero, our code pauses to run a randomized polynomial identity check \cite{schwartz1980fast} and throws an exception if a nonzero is not found.
The identity test evaluates the polynomial on 20 random points; this produces a false positive with probability under $10^{-171}$ (sufficient for the
lifetime of the code), and always reports failure for a truly zero polynomial.  The check has negligable effect on overall cost, since usually $f_1 \ne 0$.

Our geometric algorithms themselves are unremarkable: we use \cite{amenta2003incremental} for Delaunay triangulation and compute Boolean operations using axis-aligned bounding
box hierarchies to find intersections and then fire rays along the $x$-axis to get winding numbers for each contour (the $x$-axis is safe due to symbolic perturbation).  Our Delaunay
triangulation algorithm is $O(n \log n)$ for arbitrarily degenerate input, and happily computes a random but valid Delaunay triangulation if all points are at the origin.  Our current Boolean
operation algorithms degrade to $O(n^2)$ for fully degenerate input since they compute an arrangement of curves as the first step; this slowdown is independent of the perturbation
technique used, and also occurs for badly formed nondegenerate input.  Our current implementation of circular arc Booleans uses predicates of degree up to 20, but this is reducible 
to degree 12 as discussed in \cite{devillers2000algebraic}, or to degree 6 using a combination of polynomial factoring and algorithmic changes (see \TODO: Appendix?).  Even
degree 6 is problematic for Yap's scheme due to the worst case exponential blowup in the number of terms.  Finally, we note similar work in CGAL on circle arrangements in
\cite{wein2006circles}; this is orthogonal to the contribution of our work.

\TODO: Include a figure of circles

\TODO: What performance results to include?

\section{Conclusion}

We have presented a deterministic pseudorandom symbolic perturbation scheme which combines the advantages of several existing techniques.  Given a polynomial $f(x)$, we evaluate
the sign of $f(x + \epsilon_1 y_1 + \epsilon_2 y_2 + \cdots)$ where $y_k$ are deterministic pseudorandom random and $\epsilon_k$ are infinitesimals in decreasing order of size.
Typically only the first infinitesimal in this series need be considered, so our method is as fast as the linear symbolic perturbation schemes, but works for arbitrary
polynomials and appears deterministic to the caller.

\bibliography{references}
\bibliographystyle{acm}
\appendix

\section{Polynomial interpolation} \label{polynomial}

We found several useful papers discussing different aspects of univariate and multivariate polynomial interpolation, and collect these results here for convenience.

In order to recover the coefficients of $f_k(\epsilon_1, \ldots, \epsilon_k)$ we must perform multivariate interpolation given the values of $f_k$ at our chosen set of tuples.
In the univariate case, this amounts to the classical divided difference algorithm.  As discussed in \cite{oruc2000explicit} and \cite{olver2006multivariate}, the divided
difference algorithm can be beautifully expressed as the following factorization of the Vandermonde matrix into bidiagonal matrices, shown here for the degree 3 case:
\begin{equation} \label{vandermonde}
\begin{aligned}
\left(\begin{matrix}1 & x_{0} & x_{0}^{2} & x_{0}^{3}\\1 & x_{1} & x_{1}^{2} & x_{1}^{3}\\1 & x_{2} & x_{2}^{2} & x_{2}^{3}\\1 & x_{3} & x_{3}^{2} & x_{3}^{3}\end{matrix}\right)
=& \left(\begin{matrix}1&0&0&0\\\frac{1}{x_{0}-x_{1}}&\frac{1}{x_{1}-x_{0}}&0&0\\0&\frac{1}{x_{1}-x_{2}}&\frac{1}{x_{2}-x_{1}}&0\\0&0&\frac{1}{x_{2}-x_{3}}&\frac{1}{x_{3}-x_{2}}\end{matrix}\right)^{-1} \\
 & \left(\begin{matrix}1&0&0&0\\0&1&0&0\\0&\frac{1}{x_{0}-x_{2}}&\frac{1}{x_{2}-x_{0}}&0\\0&0&\frac{1}{x_{1}-x_{3}}&\frac{1}{x_{3}-x_{1}}\end{matrix}\right)^{-1} \\
 & \left(\begin{matrix}1&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&\frac{1}{x_{0}-x_{3}}&\frac{1}{x_{3}-x_{0}}\end{matrix}\right)^{-1} \\
 & \left(\begin{matrix}1&x_{0}&0&0\\0&1&x_{1}&0\\0&0&1&x_{2}\\0&0&0&1\end{matrix}\right) \\
 & \left(\begin{matrix}1&0&0&0\\0&1&x_{0}&0\\0&0&1&x_{1}\\0&0&0&1\end{matrix}\right) \\
 & \left(\begin{matrix}1&0&0&0\\0&1&0&0\\0&0&1&x_{0}\\0&0&0&1\end{matrix}\right) 
\end{aligned}
\end{equation}
This factorization was given in \cite{oruc2000explicit}, though in a somewhat less elegant form due to placing ones along the diagonal of $L$ instead of $U$ in the $LU$ factorization.
The clean $LU$ factorization was given in \cite{olver2006multivariate}, though without the further bidiagonal factorization.

The first half of this factorization is the classical divided difference algorithm to convert values $f(x_0), \ldots, f(x_k)$ into the coefficients of $f$ in the Newton
basis.  The second half expands from the Newton basis down to monomials.  In our case, we have $x_k = k$, so all of the ratios in each bidiagonal matrix have the same denominator.
In particular, we can clear fractions by multiplying the inverse by $d!$ where $d$ is the degree of $f$, after which all computations can be performed in integers.  Alternatively,
we can use the fact while the inverse of the Vandermonde matrix is not integer, both our polynomial values and the coefficients of the polynomials in both Newton and monomial basis
are integers.  It turns out that in this case all intermediate results in the divided difference algorithm are integers as well.  To show this, we must prove that the $k$th forward
difference $\Delta^k f(x)$ of an integer polynomial is divisible by $k!$.  We use the following argument due to Qiaochu Yuan\footnote{\url{http://math.stackexchange.com/questions/413600}}.
Since the transformation to and from the monomial basis to Newton basis (the second half of \autoref{vandermonde}) is integral, it suffices to check $k! \mid \Delta^k f(x)$ for an
element of the Newton basis $f(x) = x(x-1)\cdots(x-n+1) = n!\binom{x}{n}$.  Since $\Delta \binom{x}{n} = \binom{x}{n-1}$, we have
\begin{align*}
\Delta^k x(x-1)\cdots(x-(n-1))
  &= n! \binom{x}{n-k} \\
  &= \frac{n!}{(n-k)!} x(x-1)\cdots(x-(n-k-1)) \\
  &= k! \binom{n}{n-k} x(x-1)\cdots(x-(n-k-1))
\end{align*}
For the multivariate case, \cite{neidinger2009multivariable} provide an elegant generalization of the univariate divided difference algorithm when the polynomial is
evaluated on an ``easy corner'' of points, which includes the $0 \le \epsilon_i$, $\epsilon_1 + \cdots + \epsilon_k \le d$ set that we use.  All intermediate results
in their algorithm are multivariate divided differences, and are therefore integral by the above argument.  They discuss only interpolation into the multivariate Newton basis
consisting of polynomials such as
$$\prod_i x_i(x_i-1)\cdots(x_i-(n_i-1))$$
which corresponds to the first half of \autoref{vandermonde}.  The multivariate generalization of the second half of \autoref{vandermonde} is easy, since the multivariate Newton to
monomial basis transformation matrix factors into commuting matrices each expanding one variable, and these matrices are block diagonal with respect to the other variables.

\section{Circular arc predicates}

\subsection{Do two circles intersect?}

Let's work out all the predicates we need for circular arc booleans in the plane.  Let $S_i = (c_i,r_i)$ be the $i$th
circle with center $c_i$ and radius $r_i$.  Circles $S_0$ and $S_1$ intersect iff
$$ | r_1 - r_0 | < |\Delta c| < r_1 + r_0. $$
Since all quantities are positive, we can square to get
\begin{align} \label{two-circles}
(r_1 - r_0)^2 < \Delta c^2 < (r_1 + r_0)^2
\end{align}
where both inequalities are degree two polynomial predicates.

\subsection{One intersection of two circles}

Given circles $S_0, S_1$, define the intersection point $x_{01}$ as the intersection
of $S_0,S_1$ to the left of line $c_0c_1$.  We have
\begin{align*}
x_{01} &= (1-\alpha) c_0 + \alpha c_1 + \beta \Delta c^\perp \\
(x_{01} - c_i)^2 &= r_i^2 \\
x_{01}^2 - 2x_{01} \cdot c_i + c_i^2 = r_i^2.
\end{align*}
Subtracting the two circle equations gives
\begin{align*}
-2x_{01} \cdot \Delta c + c_1^2 - c_0^2 = r_1^2 - r_0^2 \\
-2c_0 \cdot \Delta c -2\alpha {\Delta c}^2 + (c_0 + c_1) \cdot \Delta c &= r_1^2 - r_0^2 \\
(1-2\alpha) {\Delta c}^2 &= r_1^2 - r_0^2 \\
1 - 2 \alpha &= \frac{r_1^2 - r_0^2}{\Delta c^2}
\end{align*}
Let
$$\gamma = \frac{1 - 2\alpha}{2} = \frac{r_1^2 - r_0^2}{2\Delta c^2}$$
so that $\alpha = 1/2 - \gamma$ and
\begin{align*}
x_{01} &= (1-1/2 + \gamma) c_0 + (1/2 - \gamma) c_1 + \beta \Delta c^\perp \\
       &= \bar{c} - \gamma \Delta c + \beta \Delta c^\perp
\end{align*}
Substituting into $S_0$'s equation gives
\begin{align*}
(x_{01} - c_0)^2 &= r_0^2 \\
\left((1/2 - \gamma) \Delta c + \beta \Delta c^\perp \right)^2 &= r_0^2 \\
(1/2 - \gamma)^2 \Delta c^2 + \beta^2 \Delta c^2 &= r_0^2 \\
\beta^2 &= \frac{r_0^2}{\Delta c^2} - (1/2 - \gamma)^2 \\
\beta^2 &= \frac{r_0^2}{\Delta c^2} - \left(\frac{\Delta c^2 - r_1^2 + r_0^2}{2 \Delta c^2}\right)^2 \\
4 \beta^2 \Delta c^4 &= 4 r_0^2 \Delta c^2 - \left( \Delta c^2 - r_1^2 + r_0^2 \right)^2
\end{align*}
To recap, the intersection between circles $S_0$ and $S_1$ is described by
\begin{align*}
x_{01} &= c_0 + \alpha \Delta c + \beta \Delta c^\perp \\
       &= \bar{c} - \gamma \Delta c + \beta \Delta c^\perp \\
\hat{\alpha} = 2 \alpha \Delta c^2 &= \Delta c^2 - r_1^2 + r_0^2 \\
\hat{\gamma} = 2 \gamma \Delta c^2 &= r_1^2 - r_0^2 \\
\hat{\beta}^2 = 4 \beta^2 \Delta c^4 &= 4 r_0^2 \Delta c^2 - \hat{\alpha}^2
\end{align*}
where $^\perp$ rotates left by $90^\circ$ and we choose the positive square root for $\beta$.

\subsection{Is the intersection of two circles inside a third?}

Substituting $x_{01}$ into the equation for circle $S_2$ gives
\begin{align*}
(x_{01} - c_2)^2 &< r_2^2 \\
(c_0 + \alpha c_{01} + \beta c_{01}^\perp - c_2)^2 &< r_2^2 \\
(\alpha c_{01} + \beta c_{01}^\perp - c_{02})^2 &< r_2^2 \\
\alpha^2 c_{01}^2 + \beta^2 c_{01}^2 + c_{02}^2 - 2 \alpha c_{01}\cdot c_{02} - 2 \beta c_{01}\times c_{02} &< r_2^2 \\
4\alpha^2 c_{01}^4 + 4\beta^2 c_{01}^4 + 4c_{01}^2 c_{02}^2 - 8 \alpha c_{01}^2 c_{01}\cdot c_{02} - 8 \beta c_{01}^2 c_{01}\times c_{02} &< 4r_2^2 c_{01}^2 \\
\hat{\alpha}^2 + \hat{\beta}^2 + 4c_{01}^2 c_{02}^2 - 4 \hat{\alpha} c_{01}\cdot c_{02} - 4 \hat{\beta} c_{01}\times c_{02} &< 4r_2^2 c_{01}^2 \\
4 r_0^2 c_{01}^2 + 4 c_{01}^2 c_{02}^2 - 4 \hat{\alpha} c_{01}\cdot c_{02} - 4 \hat{\beta} c_{01}\times c_{02} &< 4r_2^2 c_{01}^2 \\
r_0^2 c_{01}^2 + c_{01}^2 c_{02}^2 - \hat{\alpha} c_{01}\cdot c_{02} - \hat{\beta} c_{01}\times c_{02} &< r_2^2 c_{01}^2 \\
c_{01}^2 (c_{02}^2 + r_0^2 - r_2^2) - \hat{\alpha} c_{01}\cdot c_{02} - \hat{\beta} c_{01}\times c_{02} &< 0 \\
\hat{\alpha}_{02} c_{01}^2 - \hat{\alpha}_{01} c_{01}\cdot c_{02} - \hat{\beta}_{01} c_{01}\times c_{02} &< 0 \\
\hat{\alpha}_{01} c_{01}\cdot c_{02} - \hat{\alpha}_{02} c_{01}^2 + \hat{\beta}_{01} c_{01}\times c_{02} &> 0 \\
\end{align*}

\subsection{Are the intersections of two circles with a third counterclockwise?}

Alternatively, given three circles $S_0, S_1, S_2$, we ask whether $x_{01}$ occurs to the right of $x_{02}$.  Equivalently, do we have
\begin{align*}
(x_{01} - c_0) \times (x_{02} - c_0) &> 0 \\
(\alpha_1 \Delta c_1 + \beta_1 \Delta c_1^\perp) \times (\alpha_2 \Delta c_2 + \beta_2 \Delta c_2^\perp) &> 0 \\
(\alpha_1 \alpha_2 + \beta_1 \beta_2) \Delta c_1 \times \Delta c_2 + (\alpha_1 \beta_2 - \beta_1 \alpha_2) \Delta c_1 \cdot \Delta c_2 &> 0
\end{align*}
Multiply through by $4 \Delta c_1^2 \Delta c_2^2$ to clear nonconstant fractions to get
\begin{align} \label{circle-order}
(\alpha'_1 \alpha'_2 + \beta'_1 \beta'_2) \Delta c_1 \times \Delta c_2 + (\alpha'_1 \beta'_2 - \beta'_1 \alpha'_2) \Delta c_1 \cdot \Delta c_2 &> 0
\end{align}
This has the form
$$a_6 + a_4 \sqrt{b_4} + a_4 \sqrt{b_4} + a_2 \sqrt{b_4} \sqrt{b_4}$$
where subscript $k$ denotes degree $k$.  This matches lemma (5) from "On the Degree of Standard Geometric Predicates for Line Transversals in 3D", so the final predicate degree is 24.

Finally, we need a much simpler version of (\ref{circle-order}) where the second direction is one of the coordinate axes, call it $e$:
\begin{align*}
e \times (x_{01} - c_0) &> 0 \\
e \times (\alpha \Delta c + \beta \Delta c^\perp) &> 0 \\
e \times (\hat{\alpha} \Delta c + \hat{\beta} \Delta c^\perp) &> 0 \\
\hat{\alpha} e \times \Delta c &> -\hat{\beta} e \cdot \Delta c \\
\hat{\alpha}^2 (e \times \Delta c)^2 &> \hat{\beta}^2 (e \cdot \Delta c)
\end{align*}
where the last equation is correct only for appropriate sign configurations.

\subsection{Is one circle intersection above another?}

Given four circles $S_0$ through $S_3$, is $x_{01}$ below $x_{23}$?  This predicate has the form
\begin{align*}
c_0^y + \alpha_{01} c_{01}^y + \beta_{01} c_{01}^x &< c_2^y + \alpha_{23} c_{23}^y + \beta_{23} c_{23}^x \\
0 &< c_{02}^y + \alpha_{23} c_{23}^y - \alpha_{01} c_{01}^y + \beta_{23} c_{23}^x - \beta_{01} c_{01}^x \\
0 &< 2 c_{02}^y c_{01}^2 c_{23}^2 + \hat{\alpha}_{23} c_{23}^y c_{01}^2 - \hat{\alpha}_{01} c_{01}^y c_{23}^2 + \hat{\beta}_{23} c_{23}^x c_{01}^2 - \hat{\beta}_{01} c_{01}^x c_{23}^2
\end{align*}

\subsection{Does the horizontal line through one circle intersection intersect another circle?}

In other words, given circles $S_0, S_1, S_2$, do we have
\begin{align*}
c_2^y - r_2 &< c_0^y + \alpha c_{01}^y + \beta c_{01}^x < c_2^y + r_2 \\
2 (c_{02}^y - r_2) c_{01}^2 &< \hat{\alpha} c_{01}^y + \hat{\beta} c_{01}^x < 2 (c_{02}^y + r_2) c_{01}^2
\end{align*}

\subsection{Is the intersection of a horizontal line through one circle intersection with another circle to the right of the first intersection?}

Given three circles $S_0, S_1, S_2$, where the horizontal line through $x_{01}$ intersects $S_2$ at $p$, we ask whether $x_{01}^x < p^x$.  We have
\begin{align*}
p^y &= x_{01}^x \\
(p - c_2)^2 &= r_2^2 \\
(p^x - c_2^x)^2 + (x_{01}^y - c_2^y)^2 &= r_2^2 \\
p^x - c_2^x &= s \sqrt{r_2^2 - (x_{01}^y - c_2^y)^2} \\
p^x &= c_2^x + s \sqrt{r_2^2 - (x_{01}^y - c_2^y)^2}
\end{align*}
where $s = \pm 1$ depending on which intersection we're considering.  Our predicate is
\begin{align*}
x_{01}^x < c_2^x + s \sqrt{r_2^2 - (x_{01}^y - c_2^y)^2} \\
x_{01}^x - c_2^x < s \sqrt{r_2^2 - (x_{01}^y - c_2^y)^2}
\end{align*}
Assuming both sides are positive, squaring gives
\begin{align*}
(x_{01}^x - c_2^x)^2 &< r_2^2 - (x_{01}^y - c_2^y)^2 \\
(x_{01}^x - c_2^x)^2 + (x_{01}^y - c_2^y)^2 &< r_2^2 \\
(x_{01} - c_2)^2 &< r_2^2
\end{align*}
which is exactly the question of whether $x_{01}$ is inside $S_2$ as answered above.

Before squaring, the sign of the LHS is given by
\begin{align*}
x_{01}^x - c_2^x &> 0 \\
\alpha c_{01}^x + \beta c_{01}^y - c_2^x &> 0 \\
\hat{\alpha} c_{01}^x + \hat{\beta} c_{01}^y - 2 c_2^x c_{01}^2 &> 0
\end{align*}

\subsection{Is the angle at an intersection counterclockwise?}

Let $t_0, t_1$ be the counterclockwise tangents at intersection $x_{01}$.  The following are equivalent:
\begin{align*}
t_0 \times t_1 &> 0 \\
(x_{01} - c_0)^\perp \times (x_{01} - c_1)^\perp &> 0 \\
(x_{01} - c_0) \times (x_{01} - c_1) &> 0 \\
(x_{01} - c_0) \times (c_0 - c_1) &> 0 \\
(\alpha \Delta c + \beta \Delta c^\perp) \times \Delta c &< 0 \\
\beta \Delta c^2 &> 0 \\
\hat{\beta} &> 0
\end{align*}

Thus the angle is $> 180^\circ$ for the intersection to the left of segment $(c_0,c_1)$, $< 180^\circ$ for the intersection to the right.

\section{Circular arc polygons}

Well, those are all the predicates we need for exact circular arc constructive solid geometry.  \TODO: Are those really all we needed?

\section{Precision vs. flatness}

Unfortunately, implicit arcs using integer centers and radii are unable to represent straight lines exactly, raising an issue of precision.
For concreteness, let's assume an accuracy goal of 1 micron ($10^{-6}$ m) for a bounding box size of 1 meter.  This is a relative accuracy
requirement of $\epsilon = 10^{-6}$.  If we approximate a straight segment of length $l$ with a finite radius $r$, the maximum deviation is
\begin{align*}
\Delta &= r - \sqrt{r^2 - \left(\frac{l}{2}\right)^2} \\
       &= r - r \sqrt{1 - \frac{l^2}{4 r^2}} \\
       &\approx r - r \left(1 - \frac{l^2}{8 r^2} \right) \\
       &= \frac{l^2}{8 r}
\end{align*}
Requiring $\Delta < \epsilon l$, we have
\begin{align*}
\epsilon l &> \frac{l^2}{8 r} \\
\frac{r}{l} &> \frac{1}{8 \epsilon} \approx 10^{-5}
\end{align*}
This macroresolution requirement multiplies with the microresolution requirement, so if single segments are allowed to stretch all the way
across the domain, we'd require a total relative accuracy of $10^{-11}$,  Since $10^{-9}$ is right at the limit of what a single precision
int provides, this is impractical without switching to 64 bit.  $10^{-8}$ is probably the best we can do without running into integer overflow,
and this requires raising the integer limit to $2^{26}$ or so.  What can we get out of $10^{-8}$?  We can easily save one order of magnitude by
requiring no segment to extend more than $1/10$th of the bounding box, and then two orders of magnitude if the bounding box was assumed to be
only $10$ cm rather than 1 m.

So that's what we'll do, at least for now:
\begin{itemize}
\item Ask for a relative accuracy of $10^-5$, corresponding to 1 micron in a 10 cm box.
\item Subdivide arcs down to 10\% of the bounding box size.
\item Raise the single precision integer limit from $2^{24}$ to $2^{26}$, abandoning the ``fits within a float'' invariant.
\end{itemize}
Additionally, our first implementation will ignore the subdivision step, and therefore achieve only $10^{-4.5}$ accuracy.  All of this stuff
does leave a bit of a bad taste in one's mouth, but I'll set that aside for now.

\section{Safe quantization}

Suppose we quantize a single circular arc polygon to integer centers $c_i$ and radii $r_i$.  The contour makes sense only if adjacent circles
actually intersect, so we require
\begin{align*}
|r_{i+1} - r_i| < |\Delta c_i| < r_i + r_{i+1}
\end{align*}
While it's possible to achieve this, it'd be hard to guarantee that the intersections don't move a lot.

\section{Endpoint-based arc representation}

Just in case, let's talk about endpoint based arc representation.  For stability, we'll arrange things so that arcs subtend under $180^\circ$.
Thus, arc $(x_0,x_1,x_2)$ moves from $x_0$ to $x_2$ around the circumcircle in whichever way is fastest.

\subsection{Implicitization}

Assuming general position, which we can of course always do, the implicit equation for circle $(x_0,x_1,x_2)$ is
\begin{align*}
\det \left(\begin{array}{ccc}
  x^2 & x & 1 \\
  x_0^2 & x_0 & 1 \\
  x_1^2 & x_1 & 1 \\
  x_2^2 & x_2 & 1
\end{array} \right ) &= 0 \\
a_2 x^2 + a_1 \cdot x + a_0 &= 0
\end{align*}
where $a_2, a_1, a_0$ have degrees $2, 3, 4$, respectively.  Note: $a_2$ is twice the area of triangle $(x_0,x_1,x_2)$.  Expanding the equation for
a circle, we have
\begin{align*}
(x - c)^2 - r^2 &= 0 \\
x^2 - 2c \cdot x + c^2 - r^2 &= 0 \\
a_2 x^2 - 2a_2 c \cdot x + a_2 (c^2 - r^2) &= 0 \\
\hat{c} = 2 a_2 c &= -a_1 \\
a_2 r^2 &= a_2 c^2 - a_0 \\
\hat{r}^2 = 4 a_2^2 r^2 &= 4 a_2^2 c^2 - 4 a_2 a_0 = a_1^2 - 4 a_2 a_0 \\
\end{align*}
A rough estimate is that our $24$ degree intersection ordering predicate would become $4(6+12) = 72$ degree.

\end{document}
